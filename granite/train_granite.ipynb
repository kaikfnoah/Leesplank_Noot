{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install accelerate peft bitsandbytes transformers trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model from Hugging Face hub\n",
    "base_model = \"ibm-granite/granite-3.0-2b-instruct\"\n",
    "\n",
    "# New instruction dataset\n",
    "noot_dataset = \"UWV/Leesplank_NL_wikipedia_simplifications_preprocessed\"\n",
    "\n",
    "# Fine-tuned model\n",
    "# new_model = \"llama-2-7b-chat-guanaco\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "dataset_train = load_dataset(noot_dataset, split=\"train\")\n",
    "dataset_test = load_dataset(noot_dataset, split=\"test\")\n",
    "dataset_val = load_dataset(noot_dataset, split=\"val\")\n",
    "\n",
    "dataset_loadtime = timeit.default_timer() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = timeit.default_timer()\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, BitsAndBytesConfig\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "\n",
    "model_checkpoint = \"ibm-granite/granite-3.0-2b-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16 # if not set will throw a warning about slow speeds when training\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  model_checkpoint,\n",
    "  quantization_config=bnb_config,\n",
    "  device_map=\"auto\"\n",
    "\n",
    ")\n",
    "\n",
    "model_loadtime = timeit.default_timer() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import datasets\n",
    "\n",
    "def pirateify(batch):\n",
    "  prompts = [f\"make it sound like a pirate said this, do not include any preamble or explanation only piratify the following: {response}\" for response in batch['response']]\n",
    "  # Tokenize the inputs in batch and move them to GPU\n",
    "  inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to('cuda')\n",
    "  # Generate the pirate-like responses in batch\n",
    "  outputs = model.generate(**inputs, max_new_tokens=256, do_sample=True, top_p=0.95, temperature=0.7)\n",
    "  # Decode the generated tokens into text for each output in the batch\n",
    "  pirate_responses = []\n",
    "  for output in outputs:\n",
    "    pr = tokenizer.decode(output, skip_special_tokens=True)\n",
    "    if '\\n\\n' in pr:\n",
    "      pirate_responses.append(pr.split('\\n\\n')[-1])\n",
    "    else:\n",
    "      pirate_responses.append(pr)\n",
    "\n",
    "  # Move the outputs back to CPU (to free up GPU memory)\n",
    "  inputs = inputs.to('cpu')\n",
    "  outputs = outputs.to('cpu')\n",
    "  # Clear the GPU cache to release any unused memory\n",
    "  torch.cuda.empty_cache()\n",
    "  return {\n",
    "      'prompt': batch['prompt'],  # The original prompts (already a batch)\n",
    "      'response': pirate_responses  # The pirate responses, generated in batch\n",
    "  }\n",
    "\n",
    "\n",
    "def filter_long_examples(example):\n",
    "    prompt_tokens = tokenizer.tokenize(example['prompt'])\n",
    "    response_tokens = tokenizer.tokenize(example['response'])  # Tokenize the response\n",
    "    return len(response_tokens) <= 200 and len(prompt_tokens) <= 50\n",
    "\n",
    "# Apply the filter to both train and test splits\n",
    "train_filtered = dataset['train'].select(range(6000)).filter(filter_long_examples)\n",
    "test_filtered = dataset['test'].select(range(500)).filter(filter_long_examples)\n",
    "\n",
    "print(f\"train_filtered: {len(train_filtered)} observations\\ntest_filtered: {len(test_filtered)} observations\")\n",
    "pirate_train = train_filtered.select(range(1500)).map(pirateify, batched=True, batch_size=64)\n",
    "pirate_test = test_filtered.select(range(250)).map(pirateify, batched=True, batch_size=64)\n",
    "\n",
    "# Save the new dataset\n",
    "pirate_dataset = datasets.DatasetDict({\n",
    "    'train': pirate_train,\n",
    "    'test': pirate_test\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pirate_dataset['train'].to_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = timeit.default_timer()\n",
    "input_text = \"<|user>What does 'inheritance' mean?\\n<|assistant|>\\n\"\n",
    "\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "model_check_loadtime = timeit.default_timer() - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = timeit.default_timer()\n",
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['prompt'])):\n",
    "        text = f\"<|system|>\\nYou are a helpful assistant\\n<|user|>\\n{example['prompt'][i]}\\n<|assistant|>\\n{example['response'][i]}<|endoftext|>\"\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "response_template = \"\\n<|assistant|>\\n\"\n",
    "\n",
    "from trl import DataCollatorForCompletionOnlyLM\n",
    "\n",
    "response_template_ids = tokenizer.encode(response_template, add_special_tokens=False)[2:]\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template_ids, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "# Apply qLoRA\n",
    "qlora_config = LoraConfig(\n",
    "    r=16,  # The rank of the Low-Rank Adaptation\n",
    "    lora_alpha=32,  # Scaling factor for the adapted layers\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Layer names to apply LoRA to\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "# Initialize the SFTTrainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=6,\n",
    "    per_device_eval_batch_size=6,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=100,\n",
    "    fp16=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "max_seq_length = 250\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=pirate_dataset['train'],\n",
    "    eval_dataset=pirate_dataset['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    peft_config = qlora_config,\n",
    "    formatting_func=formatting_prompts_func,\n",
    "    data_collator=collator,\n",
    "    max_seq_length=max_seq_length,\n",
    ")\n",
    "\n",
    "training_setup_loadtime = timeit.default_timer() - start_time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
